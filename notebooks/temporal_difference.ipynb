{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Temporal-Difference Learning\n",
    "\n",
    "This notebook summarizes the Temporal-Difference Learning Reinforcement Learning algorithm, as described in David Silvers' Reinforcement Learning course. For each part the relevant theory will be introduced, followed by an implementation in Python.\n",
    "\n",
    "We will test our implementation by following the assignment, Easy21, which can be found at: [https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf](https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf)\n",
    "\n",
    "It covers:\n",
    "* Temporal-Difference Policy Evaluation, both state and state-action (Lecture 3)\n",
    "* Temporal-Difference Policy Iteration (Lecture 4)\n",
    "* Model-Free control using Temporal-Difference & $\\epsilon$-greedy policy improvement\n",
    "\n",
    "Previous knowledge:\n",
    "* Lectures 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Glossary\n",
    "\n",
    "* MDP: Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Implementing our environment\n",
    "\n",
    "The assignment Easy21 describes a game similar to Blackjack. We will implement the environment of the game, which allow us to use our Monte Carlo Agent to play the game!\n",
    "\n",
    "From the assignment: \"You should write an environment that implements the game Easy21. Specifically, write a function, named step, which takes as input a state s (dealer’s first card 1–10 and the player’s sum 1–21), and an action a (hit or stick), and returns a sample of the next state $s_0$ (which may be terminal if the game is finished) and reward r. We will be using this environment for model-free reinforcement learning, and you should not explicitly represent the transition matrix for the MDP. There is no discounting (γ = 1). You should treat the dealer’s moves as part of the environment, i.e. calling step with a stick action will play out the dealer’s cards and return the final reward and terminal state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "STICK = 0\n",
    "HIT = 1\n",
    "ACTION_SPACE = [\n",
    "    STICK,\n",
    "    HIT,\n",
    "]\n",
    "DEALER_HIT_MAX = 17\n",
    "\n",
    "# min in inclusive, max is exclusive!\n",
    "Range = namedtuple('Range', ['min', 'max'])\n",
    "PLAYER_RANGE_FOR_ACTION = Range(1, 22)\n",
    "DEALER_RANGE_FOR_ACTION = Range(1, 11)\n",
    "\n",
    "\n",
    "class Easy21:\n",
    "    def __init__(self):\n",
    "        self.action_space = [\n",
    "            STICK,\n",
    "            HIT,\n",
    "        ]\n",
    "        self.state_max_bound = (\n",
    "            DEALER_RANGE_FOR_ACTION.max,\n",
    "            PLAYER_RANGE_FOR_ACTION.max,\n",
    "        )\n",
    "\n",
    "        self.state_min_bound = (\n",
    "            DEALER_RANGE_FOR_ACTION.min,\n",
    "            PLAYER_RANGE_FOR_ACTION.min,\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state:\n",
    "        \"At the start of the game both the player and the dealer draw one black\n",
    "        card (fully observed)\"\n",
    "\n",
    "        :return: Observation consisting of (dealer card, player card)\n",
    "        \"\"\"\n",
    "        return tuple(np.random.randint(low=1, high=11, size=(2,)))\n",
    "\n",
    "    def _draw(self) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        Draw a card according to easy21 rules:\n",
    "        - Each draw from the deck results in a value between 1 and 10 (uniformly distributed)\n",
    "          with a colour of red (probability 1/3) or black (probability 2/3).\n",
    "        - There are no aces or picture (face) cards in this game\n",
    "        :return: Tuple of card value and card color\n",
    "        \"\"\"\n",
    "        card = np.random.randint(1, 11)\n",
    "        color = np.random.choice(['b', 'r'], p=[2/3, 1/3])\n",
    "\n",
    "        return card, color\n",
    "\n",
    "    def _draw_and_update(self, prev_sum):\n",
    "        \"\"\"\n",
    "        Draw a card from the deck and add or subtract from the current sum of the given cards\n",
    "        The values of the cards are added (black cards) or subtracted (red cards)\n",
    "\n",
    "        :param prev_sum: Previous sum of cards\n",
    "        :return: New sum of cards after drawing a card from the deck\n",
    "        \"\"\"\n",
    "        value, color = self._draw()\n",
    "        value = -1*value if color == 'r' else value\n",
    "        return prev_sum + value\n",
    "\n",
    "    def step(self, s: Tuple[int, int], a: int) -> Tuple[Tuple[int, int], int, bool]:\n",
    "        \"\"\"\n",
    "        Takes as input a state s (dealer’s first card 1–10 and the player’s sum 1–21),\n",
    "        and an action a (hit or stick), and returns a sample of the next state s0\n",
    "        (which may be terminal if the game is finished) and reward r\n",
    "\n",
    "        :param s: Input state s of format (dealer’s first card 1-10, player’s sum 1–21)\n",
    "        :param a: Action to perform: 0-stick or 1-hit\n",
    "        :return: The next state, the reward and True/False if terminal state\n",
    "        \"\"\"\n",
    "\n",
    "        dealer_card, player_sum = s\n",
    "\n",
    "        # Player sticks\n",
    "        if STICK == a:\n",
    "            s1, reward, is_terminal = self._stick(dealer_card, player_sum)\n",
    "        # Player hits\n",
    "        elif HIT == a:\n",
    "            s1, reward, is_terminal = self._hit(dealer_card, player_sum)\n",
    "        else:\n",
    "            raise ValueError('Unknown action value:', a)\n",
    "\n",
    "        # Clip state to boundaries\n",
    "        s1 = tuple(np.clip(s1, a_min=self.state_min_bound, a_max=self.state_max_bound))\n",
    "\n",
    "        return s1, reward, is_terminal\n",
    "\n",
    "    def _stick(self, dealer_card: int, player_sum: int) -> Tuple[Tuple[int, int], int, bool]:\n",
    "        \"\"\"\n",
    "        If the player sticks then the dealer starts taking turns. The dealer always\n",
    "        sticks on any sum of 17 or greater, and hits otherwise.\n",
    "        If the dealer goes bust, then the player wins; otherwise, the outcome – win (reward +1),\n",
    "        lose (reward -1), or draw (reward 0) – is the player with the largest sum.\n",
    "\n",
    "        :param dealer_card: The dealers' card, between 1-10\n",
    "        :param player_sum: The sum of the players' cards, between 1-21\n",
    "        :return: The next state, the reward and True/False if terminal state\n",
    "        \"\"\"\n",
    "\n",
    "        # Dealer hits until reaching sum of 17 or greater\n",
    "        dealer_sum = self._draw_and_update(dealer_card)\n",
    "        # while 0 < dealer_sum < DEALER_HIT_MAX:\n",
    "        while dealer_sum < DEALER_HIT_MAX:\n",
    "            dealer_sum = self._draw_and_update(dealer_sum)\n",
    "\n",
    "        # Dealer didn't go bust, winner has higher sum\n",
    "        if 1 <= dealer_sum <= 21:\n",
    "            if player_sum > dealer_sum:  # Player wins\n",
    "                return (dealer_sum, player_sum), 1, True\n",
    "            elif player_sum == dealer_sum:  # Tie\n",
    "                return (dealer_sum, player_sum), 0, True\n",
    "            else:  # Dealer wins!\n",
    "                return (dealer_sum, player_sum), -1, True\n",
    "        else:  # Dealer goes bust\n",
    "            return (dealer_sum, player_sum), 1, True\n",
    "\n",
    "    def _hit(self, dealer_card: int, player_sum: int) -> Tuple[Tuple[int, int], int, bool]:\n",
    "        \"\"\"\n",
    "        If the player’s sum exceeds 21, or becomes less than 1, then she “goes bust” and loses the game (reward -1)\n",
    "\n",
    "        :param dealer_card: The dealers' card, between 1-10\n",
    "        :param player_sum: The sum of the players' cards, between 1-21\n",
    "        :return: The next state, the reward and True/False if terminal state\n",
    "        \"\"\"\n",
    "        new_player_sum = self._draw_and_update(player_sum)\n",
    "\n",
    "        # Player still not bust yet\n",
    "        if 1 <= new_player_sum <= 21:\n",
    "            return (dealer_card, new_player_sum), 0, False\n",
    "        else:\n",
    "            return (dealer_card, new_player_sum), -1, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In addition, we will also implement a method for running a full episode in our environment. For Monte-Carlo we will need to run many full episodes, so we'll implement this once and use it for all of our Monte-Carlo implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def episode(env, policy: Callable) -> Tuple[List[Tuple[int, int]], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Run an episode in the given environment using the given policy\n",
    "\n",
    "    :param env: Environment to run in, should have step and reset methods\n",
    "    :param policy: Policy function, which given a state returns the action to perform\n",
    "    :return: List of the states, list of the rewards and list of the actions\n",
    "    \"\"\"\n",
    "    s0 = env.reset()\n",
    "\n",
    "    episode_states, rewards, actions = [], [], []\n",
    "\n",
    "    while True:\n",
    "        a = policy(s0)\n",
    "        s1, reward, is_terminal = env.step(s0, a)\n",
    "\n",
    "        episode_states.append(s0)\n",
    "        rewards.append(reward)\n",
    "        actions.append(a)\n",
    "\n",
    "        if is_terminal is True:\n",
    "            break\n",
    "\n",
    "        s0 = s1\n",
    "\n",
    "    return episode_states, rewards, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's also define a variable for how many episodes we want to run each algorithm for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "N_EPISODES = 100_000\n",
    "rng = np.random.default_rng()\n",
    "PLOT_SIZE = (7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# What is Temporal-Difference Reinforcement Learning?\n",
    "\n",
    "- TD methods learn directly from episodes of experience\n",
    "- TD is model-free: no knowledge of MDP transitions / rewards\n",
    "- TD learns from incomplete episodes, by bootstrapping \n",
    "\t- Bootstrapping: Taking a current guess, moving partially through an episode and updating the previous guess with the new guess\n",
    "- TD updates a guess towards a guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference vs. Monte-Carlo Pros and Cons\n",
    "\n",
    "TD can learn *before* knowing the final outcome\n",
    "\t- TD can learn online after every step\n",
    "\t- MC must wait until end of episode before return is known \n",
    "- TD can learn *without* the final outcome\n",
    "\t- TD can learn from incomplete sequences\n",
    "\t- MC can only learn from complete sequences\n",
    "\t- TD works in continuing (non-terminating) environments\n",
    "\t- MC only works for episodic (terminating) environments\n",
    "- MC has high variance, zero bias\n",
    "\t- Good convergence properties\n",
    "\t- (even with function approximation)\n",
    "\t- Not very sensitive to initial value\n",
    "\t- Very simple to understand and use\n",
    "- TD has low variance, some bias\n",
    "\t- Usually more efficient than MC\n",
    "\t- TD(0) converges to $v_π(s)$\n",
    "\t- (but not always with function approximation)\n",
    "\t- More sensitive to initial value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "* Goal: learn $v_π$ from episodes of experience under policy $π$\n",
    "$$S_1, A_1, R_2, ..., S_k ∼ π$$\n",
    "* Recall that the return is the total discounted reward ($\\gamma$ is discount factor):\n",
    "$$G_t = R_{t+1} + γR_{t+2} + ... + γ^{T−1}R_T$$\n",
    "- Recall that the value function is the expected return:\n",
    "$$v_π(s) = E_π[G_t | S_t = s]$$\n",
    "- Monte-Carlo policy evaluation uses empirical mean return instead of expected return\n",
    "* Robot example:\n",
    "\t* Let it talk a walk, and calculate the reward observed from the walk\n",
    "\t* Each walk is an episode\n",
    "\t* $v_\\pi=\\mathbb{E}[All walks]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from src.helpers import plot_value_function\n",
    "from src.td_agent import SARSAAgent\n",
    "\n",
    "env = Easy21()\n",
    "sarsa = SARSAAgent(env = env, lambda_=1.0)\n",
    "sarsa.learn(n_episodes=N_EPISODES)\n",
    "plot_value_function(sarsa.Q, state_action_value_function=True, figsize=PLOT_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
